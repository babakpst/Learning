{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZomkmp8wp4b"
   },
   "source": [
    "Below two lines install and load the extension to run CUDA code in Jupyter cells. Also, we need to make sure that GPU (T4) is selected as the hardware accelerator in Runtime -> Change runtime type.\n",
    "\n",
    "For details please see: https://medium.com/@iphoenix179/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEIba1yZtjhs"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zezEUcP-Laza"
   },
   "source": [
    "You can check the GPU type  assigned to you by running nvidia-smi. Please specify the GPU type used for the performance measurements in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv3Q6oYPLdi5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CXStwc5x-iM"
   },
   "source": [
    "The assignment code is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-xxSM6Mrdot"
   },
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <limits.h>\n",
    "\n",
    "bool checkResults (float *gold, float *d_data, int dimx, int dimy, float rel_tol) {\n",
    "  for (int iy = 0; iy < dimy; ++iy) {\n",
    "    for (int ix = 0; ix < dimx; ++ix) {\n",
    "      int idx = iy * dimx + ix;\n",
    "\n",
    "      float gdata = gold[idx];\n",
    "      float ddata = d_data[idx];\n",
    "\n",
    "      if (isnan(gdata) || isnan(ddata)) {\n",
    "        printf(\"Nan detected: gold %f, device %f\\n\", gdata, ddata);\n",
    "        return false;\n",
    "      }\n",
    "\n",
    "      float rdiff;\n",
    "      if (fabs(gdata) == 0.f)\n",
    "        rdiff = fabs(ddata);\n",
    "      else\n",
    "        rdiff = fabs(gdata - ddata) / fabs(gdata);\n",
    "\n",
    "      if (rdiff > rel_tol) {\n",
    "        printf(\"Error solutions don't match at iy=%d, ix=%d.\\n\", iy, ix);\n",
    "        printf(\"gold: %f, device: %f\\n\", gdata, ddata);\n",
    "        printf(\"rdiff: %f\\n\", rdiff);\n",
    "        return false;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return true;\n",
    "}\n",
    "\n",
    "void computeCpuResults(float *g_data, int dimx, int dimy, int niterations,\n",
    "                       int nreps) {\n",
    "  for (int r = 0; r < nreps; r++) {\n",
    "    printf(\"Rep: %d\\n\", r);\n",
    "#pragma omp parallel for\n",
    "    for (int iy = 0; iy < dimy; ++iy) {\n",
    "      for (int ix = 0; ix < dimx; ++ix) {\n",
    "        int idx = iy * dimx + ix;\n",
    "\n",
    "        float value = g_data[idx];\n",
    "\n",
    "        for (int i = 0; i < niterations; i++) {\n",
    "          if (ix % 4 == 0) {\n",
    "            value += sqrtf(logf(value) + 1.f);\n",
    "          } else if (ix % 4 == 1) {\n",
    "            value += sqrtf(cosf(value) + 1.f);\n",
    "          } else if (ix % 4 == 2) {\n",
    "            value += sqrtf(sinf(value) + 1.f);\n",
    "          } else if (ix % 4 == 3) {\n",
    "            value += sqrtf(tanf(value) + 1.f);\n",
    "          }\n",
    "        }\n",
    "        g_data[idx] = value;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "__global__ void kernel_A(float *g_data, int dimx, int dimy, int niterations) {\n",
    "  for (int iy = blockIdx.y * blockDim.y + threadIdx.y; iy < dimy;\n",
    "       iy += blockDim.y * gridDim.y) {\n",
    "    for (int ix = blockIdx.x * blockDim.x + threadIdx.x; ix < dimx;\n",
    "         ix += blockDim.x * gridDim.x) {\n",
    "      int idx = iy * dimx + ix;\n",
    "\n",
    "      float value = g_data[idx];\n",
    "\n",
    "      for (int i = 0; i < niterations; i++) {\n",
    "        if (ix % 4 == 0) {\n",
    "          value += sqrtf(logf(value) + 1.f);\n",
    "        } else if (ix % 4 == 1) {\n",
    "          value += sqrtf(cosf(value) + 1.f);\n",
    "        } else if (ix % 4 == 2) {\n",
    "          value += sqrtf(sinf(value) + 1.f);\n",
    "        } else if (ix % 4 == 3) {\n",
    "          value += sqrtf(tanf(value) + 1.f);\n",
    "        }\n",
    "      }\n",
    "      g_data[idx] = value;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "void launchKernel(float * d_data, int dimx, int dimy, int niterations) {\n",
    "  // Only change the contents of this function and the kernel(s). You may\n",
    "  // change the kernel's function signature as you see fit. \n",
    "\n",
    "  //query number of SMs\n",
    "  cudaDeviceProp prop;\n",
    "  cudaGetDeviceProperties(&prop, 0);\n",
    "  int num_sms = prop.multiProcessorCount;\n",
    "\n",
    "  dim3 block(1, 32);\n",
    "  dim3 grid(1, num_sms);\n",
    "  kernel_A<<<grid, block>>>(d_data, dimx, dimy, niterations);\n",
    "}\n",
    "\n",
    "float timing_experiment(float *d_data,\n",
    "                        int dimx, int dimy, int niterations, int nreps) {\n",
    "  float elapsed_time_ms = 0.0f;\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  cudaEventRecord(start, 0);\n",
    "  for (int i = 0; i < nreps; i++) {\n",
    "    launchKernel(d_data, dimx, dimy, niterations);\n",
    "  }\n",
    "  cudaEventRecord(stop, 0);\n",
    "  cudaDeviceSynchronize();\n",
    "  cudaEventElapsedTime(&elapsed_time_ms, start, stop);\n",
    "  elapsed_time_ms /= nreps;\n",
    "\n",
    "  cudaEventDestroy(start);\n",
    "  cudaEventDestroy(stop);\n",
    "\n",
    "  return elapsed_time_ms;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int dimx = 8 * 1024;\n",
    "  int dimy = 8 * 1024;\n",
    "\n",
    "  int nreps = 10;\n",
    "  int niterations = 5;\n",
    "\n",
    "  int nbytes = dimx * dimy * sizeof(float);\n",
    "\n",
    "  float *d_data = 0, *h_data = 0, *h_gold = 0;\n",
    "  cudaMalloc((void **)&d_data, nbytes);\n",
    "  if (0 == d_data) {\n",
    "    printf(\"couldn't allocate GPU memory\\n\");\n",
    "    return -1;\n",
    "  }\n",
    "  printf(\"allocated %.2f MB on GPU\\n\", nbytes / (1024.f * 1024.f));\n",
    "  h_data = (float *)malloc(nbytes);\n",
    "  h_gold = (float *)malloc(nbytes);\n",
    "  if (0 == h_data || 0 == h_gold) {\n",
    "    printf(\"couldn't allocate CPU memory\\n\");\n",
    "    return -2;\n",
    "  }\n",
    "  printf(\"allocated %.2f MB on CPU\\n\", 2.0f * nbytes / (1024.f * 1024.f));\n",
    "  for (int i = 0; i < dimx * dimy; i++) h_gold[i] = 1.0f + 0.01*(float)rand()/(float)RAND_MAX;\n",
    "  cudaMemcpy(d_data, h_gold, nbytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "  timing_experiment(d_data, dimx, dimy, niterations, 1);\n",
    "  printf(\"Verifying solution\\n\");\n",
    "\n",
    "  cudaMemcpy(h_data, d_data, nbytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  float rel_tol = .001;\n",
    "  computeCpuResults(h_gold, dimx, dimy, niterations, 1);\n",
    "  bool pass = checkResults(h_gold, h_data, dimx, dimy, rel_tol);\n",
    "\n",
    "  if (pass) {\n",
    "    printf(\"Results are correct\\n\");\n",
    "  } else {\n",
    "    printf(\"FAIL:  results are incorrect\\n\");\n",
    "  }  \n",
    "\n",
    "  float elapsed_time_ms = 0.0f;\n",
    " \n",
    "  elapsed_time_ms = timing_experiment(d_data, dimx, dimy, niterations,\n",
    "                                      nreps);\n",
    "  printf(\"A:  %8.2f ms\\n\", elapsed_time_ms);\n",
    "\n",
    "  printf(\"CUDA: %s\\n\", cudaGetErrorString(cudaGetLastError()));\n",
    "\n",
    "  if (d_data) cudaFree(d_data);\n",
    "  if (h_data) free(h_data);\n",
    "\n",
    "  cudaDeviceReset();\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
